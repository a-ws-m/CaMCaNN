A dataset of 202 surfactants was used, which was curated by a previous work
\cite{qinPredictingCriticalMicelle2021a} by accumulating results from several
publications. This dataset was selected because, to the authors' knowledge, it
is currently the largest public dataset of CMCs for several classes of
surfactant collected at standard conditions, in an aqueous environment between
\SIrange{20}{25}{\celsius}. These data were split into training and test
subsets, to simulate the real-world scenario of using a model to make inferences
about molecules for which no data is available. The training data was used to
fit the models, whilst test data was `locked away' until it had been decided
that the model was optimised, and the performance metrics on the test data was
used for comparison. For some models, the training data was further split into
optimisation and validation subsets; the optimisation data was used when
calculating the loss function during model fitting and the validation data was
used for on-the-fly evaluation of model performance during training.

To provide a consistent benchmark of model performance, the same train/test data
splits were used as \citet{qinPredictingCriticalMicelle2021a} and models were
also trained and evaluated using only the nonionic surfactants from the dataset,
so as to test whether generalised all-surfactant models can be as accurate as
models trained on one class of surfactant. The number of each class of
surfactant in the train and test subsets of the data are shown in
Table~\ref{tab:data-split}.

\begin{table}
    \centering
    \caption{The number of each class of surfactant contained in the train/test subsets of the CMC dataset.}
    \label{tab:data-split}
    \begin{tabular}{@{}llrrrr@{}} \toprule \multicolumn{2}{c}{Data subset} & \multicolumn{4}{c}{Number of}                                                    \\
               \cmidrule(r){1-2}\cmidrule(l){3-6}  Surfactant classes  & Train/test                    & Nonionics & Anionics & Cationics & Zwitterionics \\
               \midrule All                                            & Train                         & 110       & 30       & 31        & 9             \\
                                                                       & Test                          & 12        & 4        & 4         & 2             \\
               Nonionics                                               & Train                         & 98        &          &           &               \\
                                                                       & Test                          & 12        &          &           &               \\\bottomrule
    \end{tabular}
\end{table}

A QSPR pipeline requires choosing two essential functions: a representation
function, whose parameters are defined before training the model, and a
mathematical form that maps this representation to a prediction. The processes
by which these functions are developed are called \emph{feature engineering} and
\emph{model selection}, respectively.

\subsection{Feature engineering}

The ideal molecular representation depends on the task at hand. Ideally, it
should be compact, but complete
\cite{faberCrystalStructureRepresentations2015,himanenDScribeLibraryDescriptors2020};
`as simple as possible, but not simpler.' To that end, the representation should
contain enough information to distinguish between isomers that with distinct
properties, though concessions can be made if we restrict the model's domain and
self-impose limits on the type of isomers we expose the model to, both during
training and in use. Representations may also include descriptions of state,
such as temperature and pressure \cite{chenGraphNetworksUniversal2019}, but this
is redundant in cases where the training data spans a very limited range of
states.

In the case of the Stauff-Klevens model, the representation effectively has two
components: the category of the headgroup, and the length of the tail group. The
model technically distinguishes between isomers by imposing strict constraints
on the structure of the molecules to which it can be applied: position and
functional isomers correspond to distinctive categories, so that the model must
learn independent parameters for each of them, and the constraint on the tail
group means that chain isomers or the presence of non-alkyl groups in the carbon
chain are not permitted. Mathematically, this representation can be formalised using
a technique called \emph{one-hot encoding} and by defining the set of headgroups for which
we have data, $\{h_i \mid 0 \leq i \leq N\}$. The encoding is a vector given by

\begin{equation}
    \label{eq:one-hot}
    \vec{s}_i = \begin{cases}
        1 & \text{if headgroup is } h_i \\
        0 & \text{otherwise.}
    \end{cases}
\end{equation}

Different headgroups therefore correspond to orthogonal encodings. If we have a
set of trained parameters $\{A_i, B_i\}$ corresponding to headgroup $h_i$,
Equation~\ref{eq:klevens} can be rewritten as

\begin{equation}
    \log X_{cmc} = \mathbf{W}\vec{s} \cdot \begin{bmatrix}
        1 \\ -n_c
    \end{bmatrix},\quad \mathbf{W} = \begin{pmatrix}
        A_1 & A_2 & \dots  & A_N\\
        B_1 & B_2 & \dots & B_N\\
    \end{pmatrix}.
\end{equation}

We can try to make the approach more general by decomposing a molecule into
smaller sets of \emph{atomic environments} and representing it by the number of
each of these constituents. Because certain groups of atoms and bonds are common
in organic surfactants, the resulting feature vectors are not orthogonal, and we
can apply the model even when we have made small changes to the headgroup, or
introduce branching and other functional groups to the tail.

In this approach, the molecule is split into atomic environments up to a given
radius, $r$: each environment is centred on an atom and extends $r$ steps along
connecting bonds. Effectively, we discard the categorical encoding in favour of
introducing more continuous, count-based features, like $n_c$. The set of all
environments in the training data up to radius $r$, $\{e_i \mid 0 \leq i \leq
N\}$, is extracted and the resulting feature vector is

\begin{equation}
    \vec{c}_i = \text{Count}(e_i).
\end{equation}

Now, a change in headgroup composition is reflected in a change in subgraph
counts, and provided the new subgraph exists in our training data, the model can
adjust its prediction accordingly. Branch points in a carbon chain are
distinguished from main-chain groups, as they terminate in a \ce{CH} group,
rather than \ce{CH2}. This type of representation is called an
\emph{extended-connectivity fingerprint} (ECFP)
\cite{rogersExtendedConnectivityFingerprints2010}.

However, these fingerprints do not necessarily distinguish between all
positional isomers or chain isomers, particularly with smaller values of $r$,
nor are stereoisomers treated differently. Another potential disadvantage is
that the number of unique atomic environments is potentially very large relative
to the size of the data available, which poses a risk of overfitting.
Furthermore, larger environments necessarily envelop smaller ones, which means
that there is some duplicate information in the representation: the presence of
a \ce{(CH2)3} environment implies the presence of three \ce{CH2} environments,
so that there is multicollinearity. This redundancy can impede model fitting and
interpretation.

TODO: Graph representation description.

\subsection{Model selection}

TODO:
\begin{itemize}
    \item ECFP linear model and feature selection description.
    \item Graph neural network model description.
    \item Hyperband description.
\end{itemize}