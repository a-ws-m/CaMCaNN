Two datasets were used for training and testing:

\begin{description}
    \item[The Qin dataset] is a dataset of 202 surfactants, curated by a
          previous work \cite{qinPredictingCriticalMicelle2021} by accumulating
          results from several publications. To the authors' knowledge, it is
          currently the largest public dataset of CMCs for several classes of
          surfactant collected at standard conditions, in an aqueous environment
          between \SIrange{20}{25}{\celsius}.
    \item[The NIST dataset] is a dataset of 43 unique surfactants and their
          aqueous CMCs, extracted from the work of
          \citet{mukerjeeCriticalMicelleConcentrations1971}. For each
          surfactant, the mean of the experimental measurements between
          \SIrange{20}{25}{\degreeCelsius} and with no additives was used as the
          target CMC value. These data were used exclusively for testing, to
          assess whether sampling bias affects the evaluated performance.
\end{description}

The Qin data were split into training and test subsets, to simulate the real-world
scenario of using a model to make inferences about molecules for which no data
is available. The training data were used to fit the models, whilst test data
were `locked away' until it had been decided that the model was optimised, and
the performance metrics on the test data were used for comparison. For some
models, the training data were further split into optimisation and validation
subsets; the optimisation data were used when calculating the loss function
during model fitting and the validation data were used for on-the-fly evaluation
of model performance during training.

To provide a consistent benchmark of model performance, the same data splits
were used as \citet{qinPredictingCriticalMicelle2021}. The number of each class
of surfactant in the train and test subsets of the data are shown in
Table~\ref{tab:data-split}.

\begin{table}
    \centering
    \caption{The number of each class of surfactant contained in the train/test subsets of the CMC datasets.}
    \label{tab:data-split}
    \begin{tabular}{@{}llrrrr@{}} \toprule \multicolumn{2}{c}{Data subset} & \multicolumn{4}{c}{Number of}                                                    \\\cmidrule(r){1-2}\cmidrule(l){3-6}
               Dataset                                                 & Train/test                    & Nonionics & Anionics & Cationics & Zwitterionics \\\midrule
               Qin-All                                                 & Train                         & 110       & 30       & 31        & 9             \\
                                                                       & Test                          & 12        & 4        & 4         & 2             \\
               Qin-Nonionics                                           & Train                         & 110       &          &           &               \\
                                                                       & Test                          & 12        &          &           &               \\
               NIST                                                    & Train                         &           &          &           &               \\
                                                                       & Test                          & 12        & 23       & 6         & 2             \\\bottomrule
    \end{tabular}
\end{table}

A QSPR pipeline requires choosing two essential functions: a representation
function, whose parameters are defined before training the model, and a
mathematical form that maps this representation to a prediction. The processes
by which these functions are developed are called \emph{feature engineering} and
\emph{model selection}, respectively.

\subsection{Feature engineering}

The ideal molecular representation depends on the task at hand. Ideally, it
should be compact, but complete
\cite{faberCrystalStructureRepresentations2015,himanenDScribeLibraryDescriptors2020};
`as simple as possible, but not simpler.' To that end, the representation should
contain enough information to distinguish between isomers that with distinct
properties, though concessions can be made if we restrict the model's domain and
self-impose limits on the type of isomers we expose the model to, both during
training and in use. Representations may also include descriptions of state,
such as temperature and pressure \cite{chenGraphNetworksUniversal2019}, but this
is redundant in cases where the training data spans a very limited range of
states.

In the case of the Stauff-Klevens model, the representation effectively has two
components: the category of the headgroup, and the length of the tail group. The
model technically distinguishes between isomers by imposing strict constraints
on the structure of the molecules to which it can be applied: position and
functional isomers correspond to distinctive categories, so that the model must
learn independent parameters for each of them, and the constraint on the tail
group means that chain isomers or the presence of non-alkyl groups in the carbon
chain are not permitted. Mathematically, this representation can be formalised using
a technique called \emph{one-hot encoding} and by defining the set of headgroups for which
we have data, $\{h_i \mid 0 \leq i \leq N\}$. The encoding is a vector given by

\begin{equation}
    \label{eq:one-hot}
    \vec{s}_i = \begin{cases}
        1 & \text{if headgroup is } h_i \\
        0 & \text{otherwise.}
    \end{cases}
\end{equation}

Different headgroups therefore correspond to orthogonal encodings. If we have a
set of trained parameters $\{A_i, B_i\}$ corresponding to headgroup $h_i$,
Equation~\ref{eq:klevens} can be rewritten as

\begin{equation}
    \log X_{cmc} = \mathbf{W}\vec{s} \cdot \begin{bmatrix}
        1 \\ -n_c
    \end{bmatrix},\quad \mathbf{W} = \begin{pmatrix}
        A_1 & A_2 & \dots & A_N \\
        B_1 & B_2 & \dots & B_N \\
    \end{pmatrix}.
\end{equation}

We can try to make the approach more general by decomposing a molecule into
smaller sets of \emph{atomic environments} and representing it by the number of
each of these constituents. Because certain groups of atoms and bonds are common
in organic surfactants, the resulting feature vectors are not orthogonal, and we
can apply the model even when we have made small changes to the headgroup, or
introduce branching and other functional groups to the tail.

\subsubsection{Extended-connectivity fingerprints}

In this approach, the molecule is split into atomic environments up to a given
radius, $r$: each environment is centred on an atom and extends $r$ steps along
connecting bonds. Effectively, we discard the categorical encoding in favour of
introducing more continuous, count-based features, like $n_c$. The set of all
environments in the training data up to radius $r$, $\{e_i \mid 0 \leq i \leq
    N\}$, is extracted and the resulting feature vector is

\begin{equation}
    \label{eq:ecfp}
    \vec{c}_i = \text{Count}(e_i).
\end{equation}

Now, a change in headgroup composition is reflected in a change in subgraph
counts, and provided the new subgraph exists in our training data, the model can
adjust its prediction accordingly. Branch points in a carbon chain are
distinguished from main-chain groups, as they terminate in a \ce{CH} group,
rather than \ce{CH2}. This type of representation is called an
\emph{extended-connectivity fingerprint} (ECFP)
\cite{rogersExtendedConnectivityFingerprints2010}.

However, these fingerprints do not necessarily distinguish between all
positional isomers or chain isomers, particularly with smaller values of $r$,
nor are stereoisomers treated differently. Another potential disadvantage is
that the number of unique atomic environments is potentially very large relative
to the size of the data available, which poses a risk of overfitting.
Furthermore, larger environments necessarily envelop smaller ones, which means
that there is some duplicate information in the representation: the presence of
a \ce{(CH2)3} environment implies the presence of three \ce{CH2} environments,
so that there is multicollinearity. This redundancy can impede model fitting and
interpretation.

\subsubsection{Molecular graph representation}

Both of the approaches described so far rely on molecular feature vectors that
cannot describe the molecule's topology. A molecular graph is a structure that
achieves this, and it is a popular choice for cheminformatics as well as
visualisation of molecular structure. In this approach, each atom is considered
a \emph{node} and each bond an \emph{edge}. Rather than having a single feature
vector to describe the molecule as a whole, each atom is assigned its own
feature vector based on properties such as its element, hybridisation state,
charge, etc. These feature vectors are concatenated into a node feature matrix.
The graph's structure is then defined by a binary adjacency matrix.

TODO: Graph representation maths and description.

\subsection{Model selection}

\subsubsection{ECFP model}

Based on the prior knowledge encoded in Equation~\ref{eq:klevens}, it is
reasonable to assume that certain atomic environments have a linear relationship
to $\log X_{cmc}$. It therefore seems justified to apply a linear model to the
ECFP fingerprints described in Equation~\ref{eq:ecfp}. However, the issues of
the large feature vector size and multicollinearity must be addressed; a na\"ive
fit using ordinary least squares (OLS) would likely produce poor results. To
that end, a process of \emph{feature selection} was applied, whereby a subset of
the atomic environments were selected for use in the model. There are several
approaches to feature selection\cite{liFeatureSelectionData2017}; here, we chose
an approach based on \emph{regularisation}.

In this approach, we include a term in the loss function that depends on the
norm of the learned parameters, or `weights', $\vec{w}$. The two types of constraints
considered in this paper are $\ell_1$ and $\ell_2$ regularisation, which correspond to
the inclusion of $\ell_1$ and $\ell_2$ norms, respectively. By combining $\ell_1$
regularisation with the least squares regressor, we obtain the least absolute
shrinkage and selection operator (LASSO)\cite{tibshiraniRegressionShrinkageSelection1996}:

\begin{equation}
    \min_{\vec{w}} { \frac{1}{2n_{\text{samples}}} \left \Vert \mathbf{X}\vec{w} - \vec{y} \right \Vert_2 ^ 2 + \alpha \left \Vert \vec{w} \right \Vert_1},
\end{equation}
where $n_{\text{samples}}$ is the number of training samples; $\mathbf{X}$ are
the training data feature vectors, stacked row-wise into a matrix; $\vec{y}$ are
the true values of $\log X_{cmc}$; and $\alpha$ is a user-defined hyperparameter
describing the degree of regularisation ($\alpha \geq 0$). By imposing the $\ell_1$ penalty, the
model is biased towards learning a \emph{sparse} weight vector: many of its
elements will be negligible. The corresponding features can be removed from the
representation.

However, LASSO has two major flaws that make it inappropriate for the task of
ECFP regression:

\begin{itemize}
    \item The number of unique atomic environments is greater than the size of
          the training data, but LASSO will select at most $n_\text{samples}$ features
          \cite{efronLeastAngleRegression2004}. Therefore, some important environments
          may still be excluded.
    \item LASSO tends to select only one of a group of highly correlated
          variables, when there is no reason why that particular one should be
          prioritised \cite{zouRegularizationVariableSelection2005}. This is
          undesirable because we want to include both large and small atomic
          environments, despite their large correlation, and we might be misled into
          thinking that some highly correlated environments have no effect on CMC.
\end{itemize}

ElasticNet addresses these issues by imposing an additional $\ell_2$ penalty \cite{zouRegularizationVariableSelection2005}:

\begin{equation}
    \label{eq:elastic}
    \min_{\vec{w}} { \frac{1}{2n_{\text{samples}}} \left \Vert \mathbf{X}\vec{w} - \vec{y} \right \Vert_2 ^ 2 + \alpha\rho \left \Vert \vec{w} \right \Vert_1} + \frac{\alpha(1 - \rho)}{2} \left \Vert \vec{w} \right \Vert_2^2,
\end{equation}
where $\rho$ is a user-defined hyperparameter controlling the proportions of the
regularisation terms. This removes the hard limit on the number of features that can be
selected and also exhibits the `grouping effect', whereby features with high correlation
tend to be assigned similar weights.

Because of these advantages, an ElasticNet linear regression model was applied
to predict $\log X_{cmc}$ using the ECFP features. Both hyperparameters,
$\alpha$ and $\rho$, must be defined when training the model. In order to select
the best values, $k$-fold cross-validation was employed: the training data were
partitioned into $k$ subsets of roughly equal size. $k$ models were trained
using $k-1$ subsets, and the final subset was used for validation. The average
mean-squared error of the $k$ models, evaluated on their respective validation
subsets, is the model's final score. This routine was applied for a range of
$\alpha$ and $\rho$ combinations, and the lowest scoring combination was used.
The hyperparameter search space is defined in the Supplementary Information.

\subsubsection{Molecular graph model}

TODO:
\begin{itemize}
    \item Graph neural network model description.
    \item Hyperband description.
\end{itemize}