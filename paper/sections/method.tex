A dataset of 202 surfactants was used, which was curated by a previous work
\cite{qinPredictingCriticalMicelle2021a} by accumulating results from several
publications. This dataset was selected because, to the authors' knowledge, it
is currently the largest public dataset of CMCs for several classes of
surfactant collected at standard conditions, in an aqueous environment between
\SIrange{20}{25}{\celsius}. These data were split into training and test
subsets, to simulate the real-world scenario of using a model to make inferences
about molecules for which no data is available. The training data was used to
fit the models, whilst test data was `locked away' until it had been decided
that the model was optimised, and the performance metrics on the test data was
used for comparison. For some models, the training data was further split into
optimisation and validation subsets; the optimisation data was used when
calculating the loss function during model fitting and the validation data was
used for on-the-fly evaluation of model performance during training.

To provide a consistent benchmark of model performance, the same train/test data
splits were used as \citet{qinPredictingCriticalMicelle2021a} and models were
also trained and evaluated using only the nonionic surfactants from the dataset,
so as to test whether generalised all-surfactant models can be as accurate as
models trained on one class of surfactant. The number of each class of
surfactant in the train and test subsets of the data are shown in
Table~\ref{tab:data-split}.

\begin{table}
    \centering
    \caption{The number of each class of surfactant contained in the train/test subsets of the CMC dataset.}
    \label{tab:data-split}
    \begin{tabular}{@{}llrrrr@{}} \toprule \multicolumn{2}{c}{Data subset} & \multicolumn{4}{c}{Number of}                                                    \\
               \cmidrule(r){1-2}\cmidrule(l){3-6}  Surfactant classes  & Train/test                    & Nonionics & Anionics & Cationics & Zwitterionics \\
               \midrule All                                            & Train                         & 110       & 30       & 31        & 9             \\
                                                                       & Test                          & 12        & 4        & 4         & 2             \\
               Nonionics                                               & Train                         & 110        &          &           &               \\
                                                                       & Test                          & 12        &          &           &               \\\bottomrule
    \end{tabular}
\end{table}

A QSPR pipeline requires choosing two essential functions: a representation
function, whose parameters are defined before training the model, and a
mathematical form that maps this representation to a prediction. The processes
by which these functions are developed are called \emph{feature engineering} and
\emph{model selection}, respectively.

\subsection{Feature engineering}

The ideal molecular representation depends on the task at hand. Ideally, it
should be compact, but complete
\cite{faberCrystalStructureRepresentations2015,himanenDScribeLibraryDescriptors2020};
`as simple as possible, but not simpler.' To that end, the representation should
contain enough information to distinguish between isomers that with distinct
properties, though concessions can be made if we restrict the model's domain and
self-impose limits on the type of isomers we expose the model to, both during
training and in use. Representations may also include descriptions of state,
such as temperature and pressure \cite{chenGraphNetworksUniversal2019}, but this
is redundant in cases where the training data spans a very limited range of
states.

In the case of the Stauff-Klevens model, the representation effectively has two
components: the category of the headgroup, and the length of the tail group. The
model technically distinguishes between isomers by imposing strict constraints
on the structure of the molecules to which it can be applied: position and
functional isomers correspond to distinctive categories, so that the model must
learn independent parameters for each of them, and the constraint on the tail
group means that chain isomers or the presence of non-alkyl groups in the carbon
chain are not permitted. Mathematically, this representation can be formalised using
a technique called \emph{one-hot encoding} and by defining the set of headgroups for which
we have data, $\{h_i \mid 0 \leq i \leq N\}$. The encoding is a vector given by

\begin{equation}
    \label{eq:one-hot}
    \vec{s}_i = \begin{cases}
        1 & \text{if headgroup is } h_i \\
        0 & \text{otherwise.}
    \end{cases}
\end{equation}

Different headgroups therefore correspond to orthogonal encodings. If we have a
set of trained parameters $\{A_i, B_i\}$ corresponding to headgroup $h_i$,
Equation~\ref{eq:klevens} can be rewritten as

\begin{equation}
    \log X_{cmc} = \mathbf{W}\vec{s} \cdot \begin{bmatrix}
        1 \\ -n_c
    \end{bmatrix},\quad \mathbf{W} = \begin{pmatrix}
        A_1 & A_2 & \dots  & A_N\\
        B_1 & B_2 & \dots & B_N\\
    \end{pmatrix}.
\end{equation}

We can try to make the approach more general by decomposing a molecule into
smaller sets of \emph{atomic environments} and representing it by the number of
each of these constituents. Because certain groups of atoms and bonds are common
in organic surfactants, the resulting feature vectors are not orthogonal, and we
can apply the model even when we have made small changes to the headgroup, or
introduce branching and other functional groups to the tail.

In this approach, the molecule is split into atomic environments up to a given
radius, $r$: each environment is centred on an atom and extends $r$ steps along
connecting bonds. Effectively, we discard the categorical encoding in favour of
introducing more continuous, count-based features, like $n_c$. The set of all
environments in the training data up to radius $r$, $\{e_i \mid 0 \leq i \leq
N\}$, is extracted and the resulting feature vector is

\begin{equation}
    \label{eq:ecfp}
    \vec{c}_i = \text{Count}(e_i).
\end{equation}

Now, a change in headgroup composition is reflected in a change in subgraph
counts, and provided the new subgraph exists in our training data, the model can
adjust its prediction accordingly. Branch points in a carbon chain are
distinguished from main-chain groups, as they terminate in a \ce{CH} group,
rather than \ce{CH2}. This type of representation is called an
\emph{extended-connectivity fingerprint} (ECFP)
\cite{rogersExtendedConnectivityFingerprints2010}.

However, these fingerprints do not necessarily distinguish between all
positional isomers or chain isomers, particularly with smaller values of $r$,
nor are stereoisomers treated differently. Another potential disadvantage is
that the number of unique atomic environments is potentially very large relative
to the size of the data available, which poses a risk of overfitting.
Furthermore, larger environments necessarily envelop smaller ones, which means
that there is some duplicate information in the representation: the presence of
a \ce{(CH2)3} environment implies the presence of three \ce{CH2} environments,
so that there is multicollinearity. This redundancy can impede model fitting and
interpretation.

Both of the approaches described so far rely on molecular feature vectors that
cannot describe the molecule's topology. A molecular graph is a structure that
achieves this, and it is a popular choice for cheminformatics as well as
visualisation of molecular structure. In this approach, each atom is considered
a \emph{node} and each bond an \emph{edge}. Rather than having a single feature
vector to describe the molecule as a whole, each atom is assigned its own
feature vector based on properties such as its element, hybridisation state,
charge, etc. These feature vectors are concatenated into a node feature matrix.
The graph's structure is then defined by a binary adjacency matrix.

TODO: Graph representation maths and description.

\subsection{Model selection}

Based on the prior knowledge encoded in Equation~\ref{eq:klevens}, it is
reasonable to assume that certain atomic environments have a linear relationship
to $\log X_{cmc}$. It therefore seems justified to apply a linear model to the
ECFP fingerprints described in Equation~\ref{eq:ecfp}. However, the issues of
the large feature vector size and multicollinearity must be addressed; a na\"ive
fit using ordinary least squares (OLS) would likely produce poor results. To
that end, a process of \emph{feature selection} was applied, whereby a subset of
the atomic environments were selected for use in the model. There are several
approaches to feature selection\cite{liFeatureSelectionData2017}; here, we chose
an approach based on \emph{regularisation}.

In this approach, we include a term in the loss function that depends on the
norm of the learned parameters, or `weights', $\vec{w}$. The two types of constraints
considered in this paper are $L_1$ and $L_2$ regularisation, which correspond to
the inclusion of $L_1$ and $L_2$ norms, respectively. By combining $L_1$
regularisation with the least squares regressor, we obtain the least absolute
shrinkage and selection operator (LASSO)\cite{tibshiraniRegressionShrinkageSelection1996}:

\begin{equation}
    \min_{\vec{w}} { \frac{1}{2n_{\text{samples}}} ||\mathbf{X}\vec{w} - \vec{y}||_2 ^ 2 + \alpha ||\vec{w}||_1},\quad \alpha \geq 0
\end{equation}

where $\mathbf{X}$ are the training data feature vectors, stacked row-wise into
a matrix; $\vec{y}$ are the true values of $\log X_{cmc}$; and $\alpha$ is a
user-defined hyperparameter describing the degree of regularisation. By imposing
the $L_1$ penalty, the model is biased towards learning a \emph{sparse} weight
vector: many of its elements will be negligible. The corresponding features can
be removed from the representation.

One problem with LASSO 

TODO:
\begin{itemize}
    \item ElasticNet description.
    \item Graph neural network model description.
    \item Hyperband description.
\end{itemize}