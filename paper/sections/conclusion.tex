Empirical models were applied to predict CMCs from two datasets. One dataset was partitioned into training and test data (Qin-All), and a subset of the nonionic surfactants within this data was also used as a separate prediction task
(Qin-Nonionics). The NIST dataset was collected from a different source and contained some molecules with very different chemistries than the above.

A linear model based on ECFPs demonstrated remarkably good performance, improving on a previous work \cite{qinPredictingCriticalMicelle2021} that applied a more complex GNN model, despite using a smaller number of parameters and having a much faster optimisation time. A new model was presented that improved the architecture of previous work's GNN and was capable of obtaining better performances than the ECFP model on the Qin-Nonionics task and
demonstrated a better ability to generalise to the NIST dataset.

Sensitivity analysis showed that the GNN models have a tendency to overfit when
training data samples do not adequately cover the chemical space of interest.
When using small datasets, with only a few examples of certain surfactant
classes, it may be preferable to use a simpler functional form, like the linear
ECFP model.

Finally, a surrogate model was developed by feeding the latent space representation of a molecule, learned by the GNN model, to a Gaussian process.
This yielded uncertainty estimates alongside CMC predictions. Although this model appeared to fail when applied to the Qin-Nonionics task, it yielded the best predictive performance of all of the models for the Qin-All task, as well as
providing good uncertainty estimates on the in-domain NIST test data.
This allows researchers to gauge their confidence in the model's predictions.

Finally, the kernel function that is learned in the process of training the
Gaussian process was employed to visualise the chemical space through the `eyes'
of the model. By analysing this space, it was shown that chemical intuition
could be employed to determine which molecules were likely poorly represented in
the latent space, based on the fact that they were surrounded by obviously
dissimilar molecules with different chemistry. This proves to be a useful technique
for exploring the limits of a model's applicability domain, as well as gaining insights
as to why the model yields its predictions for a given molecule.