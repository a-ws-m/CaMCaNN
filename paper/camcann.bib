@article{chenGraphNetworksUniversal2019,
  ids = {chenGraphNetworksUniversal2019a},
  title = {Graph {{Networks}} as a {{Universal Machine Learning Framework}} for {{Molecules}} and {{Crystals}}},
  author = {Chen, Chi and Ye, Weike and Zuo, Yunxing and Zheng, Chen and Ong, Shyue Ping},
  year = {2019},
  month = may,
  journal = {Chemistry of Materials},
  volume = {31},
  number = {9},
  pages = {3564--3572},
  publisher = {{American Chemical Society}},
  issn = {0897-4756},
  doi = {10.1021/acs.chemmater.9b01294},
  abstract = {Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Graph Network (MEGNet) models for accurate property prediction in both molecules and crystals. We demonstrate that the MEGNet models outperform prior ML models such as the SchNet in 11 out of 13 properties of the QM9 molecule data set. Similarly, we show that MEGNet models trained on {$\sim$}60 000 crystals in the Materials Project substantially outperform prior ML models in the prediction of the formation energies, band gaps, and elastic moduli of crystals, achieving better than density functional theory accuracy over a much larger data set. We present two new strategies to address data limitations common in materials science and chemistry. First, we demonstrate a physically intuitive approach to unify four separate molecular MEGNet models for the internal energy at 0 K and room temperature, enthalpy, and Gibbs free energy into a single free energy MEGNet model by incorporating the temperature, pressure, and entropy as global state inputs. Second, we show that the learned element embeddings in MEGNet models encode periodic chemical trends and can be transfer-learned from a property model trained on a larger data set (formation energies) to improve property models with smaller amounts of data (band gaps and elastic moduli).},
  file = {C\:\\Users\\Alexander\\Zotero\\storage\\ELIBTF2Q\\Chen et al. - 2019 - Graph Networks as a Universal Machine Learning Fra.pdf;C\:\\Users\\Alexander\\Zotero\\storage\\YLE3IXD2\\Chen et al. - 2019 - Graph Networks as a Universal Machine Learning Fra.pdf}
}

@article{efronLeastAngleRegression2004,
  title = {Least Angle Regression},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  year = {2004},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {32},
  number = {2},
  pages = {407--499},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053604000000067},
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
  keywords = {62J07,boosting,coefficient paths,Lasso,Linear regression,Variable selection}
}

@article{faberCrystalStructureRepresentations2015,
  ids = {faberCrystalStructureRepresentations2015a},
  title = {Crystal Structure Representations for Machine Learning Models of Formation Energies},
  author = {Faber, Felix and Lindmaa, Alexander and von Lilienfeld, O. Anatole and Armiento, Rickard},
  year = {2015},
  journal = {International Journal of Quantum Chemistry},
  volume = {115},
  number = {16},
  pages = {1094--1101},
  issn = {1097-461X},
  doi = {10.1002/qua.24917},
  abstract = {We introduce and evaluate a set of feature vector representations of crystal structures for machine learning (ML) models of formation energies of solids. ML models of atomization energies of organic molecules have been successful using a Coulomb matrix representation of the molecule. We consider three ways to generalize such representations to periodic systems: (i) a matrix where each element is related to the Ewald sum of the electrostatic interaction between two different atoms in the unit cell repeated over the lattice; (ii) an extended Coulomb-like matrix that takes into account a number of neighboring unit cells; and (iii) an ansatz that mimics the periodicity and the basic features of the elements in the Ewald sum matrix using a sine function of the crystal coordinates of the atoms. The representations are compared for a Laplacian kernel with Manhattan norm, trained to reproduce formation energies using a dataset of 3938 crystal structures obtained from the Materials Project. For training sets consisting of 3000 crystals, the generalization error in predicting formation energies of new structures corresponds to (i) 0.49, (ii) 0.64, and (iii) for the respective representations. \textcopyright{} 2015 Wiley Periodicals, Inc.},
  copyright = {\textcopyright{} 2015 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {crystal structure,formation energies,machine learning,periodic systems,representations},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qua.24917},
  file = {C\:\\Users\\Alexander\\Zotero\\storage\\7EVNJLP9\\Faber et al. - 2015 - Crystal structure representations for machine lear.pdf;C\:\\Users\\Alexander\\Zotero\\storage\\UECJ5P7F\\Faber et al. - 2015 - Crystal structure representations for machine lear.pdf}
}

@article{gaudinNewQSPRModels2016,
  title = {New {{QSPR Models}} to {{Predict}} the {{Critical Micelle Concentration}} of {{Sugar-Based Surfactants}}},
  author = {Gaudin, Th{\'e}ophile and Rotureau, Patricia and Pezron, Isabelle and Fayet, Guillaume},
  year = {2016},
  month = nov,
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {55},
  number = {45},
  pages = {11716--11726},
  publisher = {{American Chemical Society}},
  issn = {0888-5885},
  doi = {10.1021/acs.iecr.6b02890},
  abstract = {Sugar-based surfactants represent a fruitful field of research in the context of sustainable chemistry since they can be obtained from renewable resources. In this work, new quantitative structure property relationships (QSPR) models for the critical micelle concentration (CMC) dedicated to sugar-based surfactants are proposed in order to reduce testing in a screening perspective. An important literature compilation allowed the constitution of a data set of 83 sugar-based surfactants for which accurate CMC values were found. Then, a series of QSPR models were developed based on molecular descriptors of the whole molecule and of the hydrophobic and hydrophilic fragments taken separately. Different models were considered by including quantum-chemical descriptors with hope to access physically based models and by using only simple constitutional descriptors to favor fast and easy prediction. The best QSPR model was obtained including quantum-chemical descriptors of the whole molecular structure with a root-mean-square error (RMSE) of 0.32 (log) evaluated on a validation set of 27 molecules. A simpler model with good performances was also found (with a RMSE of 0.36 (log) on the validation set), including only constitutional-based fragment descriptors, that can be easily computed from the 2-dimension structure of the hydrophilic and hydrophobic fragments.},
  file = {C\:\\Users\\Alexander\\Zotero\\storage\\6YLTM8SC\\Gaudin et al. - 2016 - New QSPR Models to Predict the Critical Micelle Co.pdf}
}

@misc{grattarolaGraphNeuralNetworks2020,
  title = {Graph {{Neural Networks}} in {{TensorFlow}} and {{Keras}} with {{Spektral}}},
  author = {Grattarola, Daniele and Alippi, Cesare},
  year = {2020},
  month = jun,
  number = {arXiv:2006.12138},
  eprint = {2006.12138},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.12138},
  abstract = {In this paper we present Spektral, an open-source Python library for building graph neural networks with TensorFlow and the Keras application programming interface. Spektral implements a large set of methods for deep learning on graphs, including message-passing and pooling operators, as well as utilities for processing graphs and loading popular benchmark datasets. The purpose of this library is to provide the essential building blocks for creating graph neural networks, focusing on the guiding principles of user-friendliness and quick prototyping on which Keras is based. Spektral is, therefore, suitable for absolute beginners and expert deep learning practitioners alike. In this work, we present an overview of Spektral's features and report the performance of the methods implemented by the library in scenarios of node classification, graph classification, and graph regression.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{haraguchiSparseModelingSmall2022,
  title = {Sparse Modeling for Small Data: Case Studies in Controlled Synthesis of {{2D}} Materials},
  shorttitle = {Sparse Modeling for Small Data},
  author = {Haraguchi, Yuri and Igarashi, Yasuhiko and Imai, Hiroaki and Oaki, Yuya},
  year = {2022},
  journal = {Digital Discovery},
  volume = {1},
  number = {1},
  pages = {26--34},
  publisher = {{Royal Society of Chemistry}},
  doi = {10.1039/D1DD00010A},
  langid = {english}
}

@article{himanenDScribeLibraryDescriptors2020,
  title = {{{DScribe}}: {{Library}} of Descriptors for Machine Learning in Materials Science},
  shorttitle = {{{DScribe}}},
  author = {Himanen, Lauri and J{\"a}ger, Marc O. J. and Morooka, Eiaki V. and Federici Canova, Filippo and Ranawat, Yashasvi S. and Gao, David Z. and Rinke, Patrick and Foster, Adam S.},
  year = {2020},
  month = feb,
  journal = {Computer Physics Communications},
  volume = {247},
  pages = {106949},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2019.106949},
  abstract = {DScribe is a software package for machine learning that provides popular feature transformations (``descriptors'') for atomistic materials simulations. DScribe accelerates the application of machine learning for atomistic property prediction by providing user-friendly, off-the-shelf descriptor implementations. The package currently contains implementations for Coulomb matrix, Ewald sum matrix, sine matrix, Many-body Tensor Representation (MBTR), Atom-centered Symmetry Function (ACSF) and Smooth Overlap of Atomic Positions (SOAP). Usage of the package is illustrated for two different applications: formation energy prediction for solids and ionic charge prediction for atoms in organic molecules. The package is freely available under the open-source Apache License 2.0. Program summary Program Title: DScribe Program Files doi: http://dx.doi.org/10.17632/vzrs8n8pk6.1 Licensing provisions: Apache-2.0 Programming language: Python/C/C++ Supplementary material: Supplementary Information as PDF Nature of problem: The application of machine learning for materials science is hindered by the lack of consistent software implementations for feature transformations. These feature transformations, also called descriptors, are a key step in building machine learning models for property prediction in materials science. Solution method: We have developed a library for creating common descriptors used in machine learning applied to materials science. We provide an implementation the following descriptors: Coulomb matrix, Ewald sum matrix, sine matrix, Many-body Tensor Representation (MBTR), Atom-centered Symmetry Functions (ACSF) and Smooth Overlap of Atomic Positions (SOAP). The library has a python interface with computationally intensive routines written in C or C++. The source code, tutorials and documentation are provided online. A continuous integration mechanism is set up to automatically run a series of regression tests and check code coverage when the codebase is updated.},
  langid = {english},
  keywords = {Descriptor,Machine learning,Materials science,Open source,Python},
  file = {C\:\\Users\\Alexander\\Zotero\\storage\\QUQC4QNT\\Himanen et al. - 2020 - DScribe Library of descriptors for machine learni.pdf}
}

@article{jakobtorweihenPredictingCriticalMicelle2017,
  title = {Predicting {{Critical Micelle Concentrations}} with {{Molecular Dynamics Simulations}} and {{COSMOmic}}},
  author = {Jakobtorweihen, Sven and Yordanova, Denitsa and Smirnova, Irina},
  year = {2017},
  journal = {Chemie Ingenieur Technik},
  volume = {89},
  number = {10},
  pages = {1288--1296},
  issn = {1522-2640},
  doi = {10.1002/cite.201700061},
  abstract = {Surfactants are amphiphilic molecules which are capable of forming micelles. Therefore, they are used in many applications. The critical micelle concentration (CMC), the surfactant concentration at which micelles start to form, is an important property of these systems. Here, the applicability of COSMOmic (an extension of COSMO-RS) to predict CMCs is reported for the first time. Molecular dynamics simulations were used as second method to calculate transfer free energies needed for the calculation of CMCs. In particular, CMCs for poly(oxyethylene) monoalkyl ether surfactants are determined.},
  langid = {english},
  keywords = {COSMO-RS,Critical micelle concentration,Molecular dynamics simulations,Surfactants},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cite.201700061},
  file = {C\:\\Users\\Alexander\\Zotero\\storage\\9PQLWKUT\\Jakobtorweihen et al. - 2017 - Predicting Critical Micelle Concentrations with Mo.pdf}
}

@article{klevensStructureAggregationDilate1953,
  title = {Structure and Aggregation in Dilate Solution of Surface Active Agents},
  author = {Klevens, H. B.},
  year = {1953},
  month = feb,
  journal = {Journal of the American Oil Chemists Society},
  volume = {30},
  number = {2},
  pages = {74--80},
  issn = {1558-9331},
  doi = {10.1007/BF02635002},
  abstract = {Critical micelle concentrations (CMC) are shown to depend on chain length. All straight chain saturated surface active agents of equal ion length have approximately the same CMC. Thus a C13 fatty acid soap, a C12 sulfonate, a C11 sulfate, and a C12 ammonium chloride have CMC values of 0.010\textendash 0.014M. Values of CMC are not changed to any extent by substitution near the hydrophilic head of one, two, or three groups, even as large as hydroxyethyl, in place of the amine hydrogens in the cationic detergents. However substitution with dihydroxypropyl groups has a marked effect on association in C12 but not in the C16 series, indicating that the relative lengths of the two chains must be considered as an important factor in association. This is well illustrated in the dialkylsulfosuccinate series, and in a tetradecane sulfate series in which the -SO4Na group was progressively moved down the chain, particularly when the branched chain compounds are compared with the corresponding straight chain detergents of length equal to the maximum length from the charged head to the ultimate carbon atom. Introduction of double bonds causes a small but definite increase in CMC whereas polar substitution in the chain results in a marked increase in CMC. Possible micelle structures are discussed in the light of these association phenomena, and it is concluded that the assignment of a definite size and shape to micelles appears at the present time to be slightly premature. However if one includes the concept of relative order-disorder in the micelle as one of the factors, in addition to chain length, type of detergent, and environment, which are important in micelle structure, it is possible to explain partially the apparent marked differences which have been reported for micelles of different surface active agents.},
  langid = {english},
  keywords = {Critical Micelle Concentration,Gossypol,Straight Chain,Surface Active Agent}
}

@article{liFeatureSelectionData2017,
  title = {Feature {{Selection}}: {{A Data Perspective}}},
  shorttitle = {Feature {{Selection}}},
  author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
  year = {2017},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {6},
  pages = {94:1--94:45},
  issn = {0360-0300},
  doi = {10.1145/3136625},
  abstract = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.},
  keywords = {Feature selection}
}

@article{liHyperbandNovelBanditBased2018,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {185},
  pages = {1--52},
  issn = {1533-7928},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, \~A\c uralg , for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare \~A\c uralg with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that \~A\c uralg can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.}
}

@article{matteiModelingCriticalMicelle2013,
  title = {Modeling of the {{Critical Micelle Concentration}} ({{CMC}}) of {{Nonionic Surfactants}} with an {{Extended Group-Contribution Method}}},
  author = {Mattei, Michele and Kontogeorgis, Georgios M. and Gani, Rafiqul},
  year = {2013},
  month = aug,
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {52},
  number = {34},
  pages = {12236--12246},
  publisher = {{American Chemical Society}},
  issn = {0888-5885},
  doi = {10.1021/ie4016232},
  abstract = {A group-contribution (GC) property prediction model for estimating the critical micelle concentration (CMC) of nonionic surfactants in water at 25 \textdegree C is presented. The model is based on the Marrero and Gani GC method. A systematic analysis of the model performance against experimental data is carried out using data for a wide range of nonionic surfactants covering a wide range of molecular structures. As a result of this procedure, new third order groups based on the characteristic structures of nonionic surfactants are defined and are included in the Marrero and Gani GC model. In this way, those compounds that exhibit larger correlation errors (based only on first- and second-order groups) are assigned to more detailed molecular descriptions, so that better correlations of critical micelle concentrations are obtained. The group parameter estimation has been performed using a data set of 150 experimental measurements covering a large variety of nonionic surfactants including linear, branched, and phenyl alkyl ethoxylates; alkanediols; alkyl mono- and disaccharide ethers and esters; ethoxylated alkyl amines and amides; fluorinated linear ethoxylates and amides; polyglycerol esters; and carbohydrate derivate ethers, esters, and thiols. The model developed consists of linear group contributions, and the critical micelle concentration is estimated using the molecular structure of the nonionic surfactant alone. Compared to other models used for the prediction of the critical micelle concentration, and in particular, the quantitative structure\textendash property relationship models, the developed GC model provides an accurate correlation and allows for an easier and faster application in computer-aided molecular design techniques facilitating chemical process and product design.}
}

@article{mobleyEscapingAtomTypes2018,
  title = {Escaping {{Atom Types}} in {{Force Fields Using Direct Chemical Perception}}},
  author = {Mobley, David L. and Bannan, Caitlin C. and Rizzi, Andrea and Bayly, Christopher I. and Chodera, John D. and Lim, Victoria T. and Lim, Nathan M. and Beauchamp, Kyle A. and Slochower, David R. and Shirts, Michael R. and Gilson, Michael K. and Eastman, Peter K.},
  year = {2018},
  month = nov,
  journal = {Journal of Chemical Theory and Computation},
  volume = {14},
  number = {11},
  pages = {6076--6092},
  publisher = {{American Chemical Society}},
  issn = {1549-9618},
  doi = {10.1021/acs.jctc.8b00640},
  abstract = {Traditional approaches to specifying a molecular mechanics force field encode all the information needed to assign force field parameters to a given molecule into a discrete set of atom types. This is equivalent to a representation consisting of a molecular graph comprising a set of vertices, which represent atoms labeled by atom type, and unlabeled edges, which represent chemical bonds. Bond stretch, angle bend, and dihedral parameters are then assigned by looking up bonded pairs, triplets, and quartets of atom types in parameter tables to assign valence terms and using the atom types themselves to assign nonbonded parameters. This approach, which we call indirect chemical perception because it operates on the intermediate graph of atom-typed nodes, creates a number of technical problems. For example, atom types must be sufficiently complex to encode all necessary information about the molecular environment, making it difficult to extend force fields encoded this way. Atom typing also results in a proliferation of redundant parameters applied to chemically equivalent classes of valence terms, needlessly increasing force field complexity. Here, we describe a new approach to assigning force field parameters via direct chemical perception. Rather than working through the intermediary of the atom-typed graph, direct chemical perception operates directly on the unmodified chemical graph of the molecule to assign parameters. In particular, parameters are assigned to each type of force field term (e.g., bond stretch, angle bend, torsion, and Lennard\textendash Jones) based on standard chemical substructure queries implemented via the industry-standard SMARTS chemical perception language, using SMIRKS extensions that permit labeling of specific atoms within a chemical pattern. We use this to implement a new force field format, called the SMIRKS Native Open Force Field (SMIRNOFF) format. We demonstrate the power and generality of this approach using examples of specific molecules that pose problems for indirect chemical perception and construct and validate a minimalist yet very general force field, SMIRNOFF99Frosst. We find that a parameter definition file only {$\sim$}300 lines long provides coverage of all but {$<$}0.02\% of a 5 million molecule drug-like test set. Despite its simplicity, the accuracy of SMIRNOFF99Frosst for small molecule hydration free energies and selected properties of pure organic liquids is similar to that of the General Amber Force Field, whose specification requires thousands of parameters. This force field provides a starting point for further optimization and refitting work to follow.}
}

@article{qinPredictingCriticalMicelle2021a,
  title = {Predicting {{Critical Micelle Concentrations}} for {{Surfactants Using Graph Convolutional Neural Networks}}},
  author = {Qin, Shiyi and Jin, Tianyi and Van Lehn, Reid C. and Zavala, Victor M.},
  year = {2021},
  month = sep,
  journal = {The Journal of Physical Chemistry B},
  volume = {125},
  number = {37},
  pages = {10610--10620},
  publisher = {{American Chemical Society}},
  issn = {1520-6106},
  doi = {10.1021/acs.jpcb.1c05264},
  abstract = {Surfactants are amphiphilic molecules that are widely used in consumer products, industrial processes, and biological applications. A critical property of a surfactant is the critical micelle concentration (CMC), which is the concentration at which surfactant molecules undergo cooperative self-assembly in solution. Notably, the primary method to obtain CMCs experimentally\textemdash tensiometry\textemdash is laborious and expensive. In this study, we show that graph convolutional neural networks (GCNs) can predict CMCs directly from the surfactant molecular structure. In particular, we developed a GCN architecture that encodes the surfactant structure in the form of a molecular graph and trained it using experimental CMC data. We found that the GCN can predict CMCs with higher accuracy on a more inclusive data set than previously proposed methods and that it can generalize to anionic, cationic, zwitterionic, and nonionic surfactants using a single model. Molecular saliency maps revealed how atom types and surfactant molecular substructures contribute to CMCs and found this behavior to be in agreement with physical rules that correlate constitutional and topological information to CMCs. Following such rules, we proposed a small set of new surfactants for which experimental CMCs are not available; for these molecules, CMCs predicted with our GCN exhibited similar trends to those obtained from molecular simulations. These results provide evidence that GCNs can enable high-throughput screening of surfactants with desired self-assembly characteristics.}
}

@misc{rittigGraphNeuralNetworks2022,
  title = {Graph Neural Networks for the Prediction of Molecular Structure-Property Relationships},
  author = {Rittig, Jan G. and Gao, Qinghe and Dahmen, Manuel and Mitsos, Alexander and Schweidtmann, Artur M.},
  year = {2022},
  month = jul,
  number = {arXiv:2208.04852},
  eprint = {2208.04852},
  eprinttype = {arxiv},
  primaryclass = {cs, math, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.04852},
  abstract = {Molecular property prediction is of crucial importance in many disciplines such as drug discovery, molecular biology, or material and process design. The frequently employed quantitative structure-property/activity relationships (QSPRs/QSARs) characterize molecules by descriptors which are then mapped to the properties of interest via a linear or nonlinear model. In contrast, graph neural networks, a novel machine learning method, directly work on the molecular graph, i.e., a graph representation where atoms correspond to nodes and bonds correspond to edges. GNNs allow to learn properties in an end-to-end fashion, thereby avoiding the need for informative descriptors as in QSPRs/QSARs. GNNs have been shown to achieve state-of-the-art prediction performance on various property predictions tasks and represent an active field of research. We describe the fundamentals of GNNs and demonstrate the application of GNNs via two examples for molecular property prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Quantitative Biology - Biomolecules}
}

@article{rogersExtendedConnectivityFingerprints2010,
  title = {Extended-{{Connectivity Fingerprints}}},
  author = {Rogers, David and Hahn, Mathew},
  year = {2010},
  month = may,
  journal = {Journal of Chemical Information and Modeling},
  volume = {50},
  number = {5},
  pages = {742--754},
  publisher = {{American Chemical Society}},
  issn = {1549-9596},
  doi = {10.1021/ci100050t},
  abstract = {Extended-connectivity fingerprints (ECFPs) are a novel class of topological fingerprints for molecular characterization. Historically, topological fingerprints were developed for substructure and similarity searching. ECFPs were developed specifically for structure-activity modeling. ECFPs are circular fingerprints with a number of useful qualities: they can be very rapidly calculated; they are not predefined and can represent an essentially infinite number of different molecular features (including stereochemical information); their features represent the presence of particular substructures, allowing easier interpretation of analysis results; and the ECFP algorithm can be tailored to generate different types of circular fingerprints, optimized for different uses. While the use of ECFPs has been widely adopted and validated, a description of their implementation has not previously been presented in the literature.}
}

@incollection{schuttQuantumChemicalInsightsInterpretable2019,
  title = {Quantum-{{Chemical Insights}} from {{Interpretable Atomistic Neural Networks}}},
  booktitle = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  author = {Sch{\"u}tt, Kristof T. and Gastegger, Michael and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert},
  editor = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {311--330},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-28954-6_17},
  abstract = {With the rise of deep neural networks for quantum chemistry applications, there is a pressing need for architectures that, beyond delivering accurate predictions of chemical properties, are readily interpretable by researchers. Here, we describe interpretation techniques for atomistic neural networks on the example of Behler\textendash Parrinello networks as well as the end-to-end model SchNet. Both models obtain predictions of chemical properties by aggregating atom-wise contributions. These latent variables can serve as local explanations of a prediction and are obtained during training without additional cost. Due to their correspondence to well-known chemical concepts such as atomic energies and partial charges, these atom-wise explanations enable insights not only about the model but more importantly about the underlying quantum-chemical regularities. We generalize from atomistic explanations to 3d space, thus obtaining spatially resolved visualizations which further improve interpretability. Finally, we analyze learned embeddings of chemical elements that exhibit a partial ordering that resembles the order of the periodic table. As the examined neural networks show excellent agreement with chemical knowledge, the presented techniques open up new venues for data-driven research in chemistry, physics and materials science.},
  isbn = {978-3-030-28954-6},
  langid = {english},
  file = {C\:\\Users\\Alexander\\Zotero\\storage\\7A6UT3WP\\Schütt et al. - 2019 - Quantum-Chemical Insights from Interpretable Atomi.pdf}
}

@article{tibshiraniRegressionShrinkageSelection1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  number = {1},
  pages = {267--288},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.}
}

@article{zouRegularizationVariableSelection2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  year = {2005},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {67},
  number = {2},
  pages = {301--320},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  abstract = {Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p{$\gg$}n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
  langid = {english},
  keywords = {Grouping effect,LARS algorithm,Lasso,p≫n problem,Penalization,Variable selection},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00503.x}
}

